{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': '/Applications/anaconda3/lib/python3.7/site-packages/sklearn/datasets/data/iris.csv'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the Dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " array(['setosa', 'versicolor', 'virginica'], dtype='<U10'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "columns_names = [\"Petal length\",\"Petal Width\",\"Sepal Length\",\"Sepal Width\"]\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "target_names = iris.target_names\n",
    "X,Y,target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the data, lets write a recursive function, That will select a feature\n",
    "# (Based on Gain Ratio). Since the data is continuous, We need to select a point in the \n",
    "# continuous data, according to which we will get the maximum Information \n",
    "import math\n",
    "\n",
    "def entropy(X,Y,num_targets):\n",
    "    ans = 0\n",
    "    num_features = X.shape[1]\n",
    "    if(Y.shape[0] == 0):\n",
    "        return 0\n",
    "    # Calculating entropy as sum(-pi log2 pi)\n",
    "    total_points = Y.shape[0]\n",
    "    for i in range(num_targets):\n",
    "        if Y[Y==i].shape[0] > 0:\n",
    "            ans -= ((Y[Y==i].shape[0] / total_points) * math.log2 ((Y[Y==i].shape[0] / total_points)))\n",
    "    return ans\n",
    "\n",
    "def DecisionTree(X,Y,features_used,feature_names,target_names,level = 0):\n",
    "    # X -> Features\n",
    "    # Y -> Target\n",
    "    # features_used -> 1D array, which is used to specify which feature has been used, 0 is not used, 1 is used\n",
    "    # feature_names -> Name of the features/columns\n",
    "    # target_names -> Name of the target classes\n",
    "    # level -> Current level of the node\n",
    "    \n",
    "    print(\"Level \", level)\n",
    "    num_features = X.shape[1]\n",
    "    count = []\n",
    "    num_targets = len(target_names)\n",
    "    # Let's get the count of all the target values\n",
    "    for i in range(num_targets):\n",
    "        count.append(Y[Y==i].shape[0])\n",
    "\n",
    "    for i in range(len(target_names)):\n",
    "        if(count[i] > 0):\n",
    "            print(\"Count of \",i,\"(\",target_names[i],\") \",\" = \", count[i],sep='')\n",
    "    print(\"Current Entropy is =\", entropy(X,Y,num_targets))\n",
    "    \n",
    "    if(entropy(X,Y,num_targets) == 0):\n",
    "        # If Entropy = 0, It means leaf node\n",
    "        print(\"Leaf Node\")\n",
    "    elif(features_used[features_used == 0].shape[0] == 0):\n",
    "        # All the features are used to split and exhausted\n",
    "        print(\"Leaf Node\")\n",
    "    else:\n",
    "        feature_idx = -1\n",
    "        max_gain_ratio = -1\n",
    "        feature_split = -1\n",
    "        # Go over all the features\n",
    "        for i in range(num_features):\n",
    "            if not features_used[i]:\n",
    "                # Since the data is continuous, We have to check all the values, And take the maximum of all the values\n",
    "                # As the values are like 3.4, 3.5, 3.6... We run the loop as 3.35, 3.45, 3.55...\n",
    "                for j in np.arange(X[:,i].min() - 0.05,  X[:,i].max() + 0.05, 0.1):\n",
    "                    # Splitting on ith index at value j\n",
    "                    # Lets get the left array, and right array\n",
    "                    X_left  = X[X[:,i] < j] \n",
    "                    X_right = X[X[:,i] > j]\n",
    "                    Y_left  = Y[X[:,i] < j]\n",
    "                    Y_right = Y[X[:,i] > j]\n",
    "                                        \n",
    "                    # Getting entropy of both the sides\n",
    "                    entropy_left = entropy(X_left,Y_left,num_targets)\n",
    "                    entropy_right = entropy(X_right,Y_right,num_targets)\n",
    "\n",
    "                    size_total = Y.shape[0]\n",
    "                    size_left = Y_left.shape[0]\n",
    "                    size_right = Y_right.shape[0]\n",
    "                    \n",
    "                    # Calculating InfoGain\n",
    "                    entropy_child = (entropy_left * size_left + entropy_right * size_right) / size_total\n",
    "                    current_entropy = entropy(X,Y,num_targets)\n",
    "                    info_gain = current_entropy - entropy_child\n",
    "                    \n",
    "                    # Calculating SplitInfo\n",
    "                    split_info = 0\n",
    "                    if size_left > 0:\n",
    "                        split_info -= (size_left/size_total) * math.log2(size_left/size_total)\n",
    "                    if size_right > 0:\n",
    "                        split_info -= (size_right/size_total) * math.log2(size_right/size_total)\n",
    "\n",
    "                    # print(feature_names[i], j, size_left, size_right, size_total, info_gain, split_info)\n",
    "\n",
    "                    # Calculating Gain Ratio\n",
    "                    if split_info > 0:\n",
    "                        gain_ratio = info_gain/split_info\n",
    "                    \n",
    "                        if gain_ratio > max_gain_ratio:\n",
    "                            feature_idx = i\n",
    "                            feature_split = j\n",
    "                            max_gain_ratio = gain_ratio\n",
    "                    \n",
    "        print('Splitting on feature',feature_names[feature_idx],'with gain ratio',round(max_gain_ratio, 3),\"at value\", round(feature_split, 3))\n",
    "        features_used[feature_idx] = 1\n",
    "        print()\n",
    "        DecisionTree(X[X[:,feature_idx] < feature_split], Y[X[:,feature_idx] < feature_split], features_used,feature_names,target_names,level + 1)\n",
    "        print()\n",
    "        DecisionTree(X[X[:,feature_idx] > feature_split], Y[X[:,feature_idx] > feature_split], features_used,feature_names,target_names,level + 1)\n",
    "        features_used[feature_idx] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level  0\n",
      "Count of 0(setosa)  = 50\n",
      "Count of 1(versicolor)  = 50\n",
      "Count of 2(virginica)  = 50\n",
      "Current Entropy is = 1.584962500721156\n",
      "Splitting on feature Sepal Length with gain ratio 1.0 at value 1.95\n",
      "\n",
      "Level  1\n",
      "Count of 0(setosa)  = 50\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node\n",
      "\n",
      "Level  1\n",
      "Count of 1(versicolor)  = 50\n",
      "Count of 2(virginica)  = 50\n",
      "Current Entropy is = 1.0\n",
      "Splitting on feature Sepal Width with gain ratio 0.693 at value 1.75\n",
      "\n",
      "Level  2\n",
      "Count of 1(versicolor)  = 49\n",
      "Count of 2(virginica)  = 5\n",
      "Current Entropy is = 0.44506485705083865\n",
      "Splitting on feature Petal length with gain ratio 0.498 at value 7.05\n",
      "\n",
      "Level  3\n",
      "Count of 1(versicolor)  = 49\n",
      "Count of 2(virginica)  = 4\n",
      "Current Entropy is = 0.3860189005698934\n",
      "Splitting on feature Petal Width with gain ratio 0.063 at value 2.85\n",
      "\n",
      "Level  4\n",
      "Count of 1(versicolor)  = 27\n",
      "Count of 2(virginica)  = 4\n",
      "Current Entropy is = 0.5547781633412736\n",
      "Leaf Node\n",
      "\n",
      "Level  4\n",
      "Count of 1(versicolor)  = 22\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node\n",
      "\n",
      "Level  3\n",
      "Count of 2(virginica)  = 1\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node\n",
      "\n",
      "Level  2\n",
      "Count of 1(versicolor)  = 1\n",
      "Count of 2(virginica)  = 45\n",
      "Current Entropy is = 0.15109697051711368\n",
      "Splitting on feature Petal length with gain ratio 0.099 at value 5.95\n",
      "\n",
      "Level  3\n",
      "Count of 1(versicolor)  = 1\n",
      "Count of 2(virginica)  = 6\n",
      "Current Entropy is = 0.5916727785823275\n",
      "Splitting on feature Petal Width with gain ratio 1.0 at value 3.05\n",
      "\n",
      "Level  4\n",
      "Count of 2(virginica)  = 6\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node\n",
      "\n",
      "Level  4\n",
      "Count of 1(versicolor)  = 1\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node\n",
      "\n",
      "Level  3\n",
      "Count of 2(virginica)  = 39\n",
      "Current Entropy is = 0.0\n",
      "Leaf Node\n"
     ]
    }
   ],
   "source": [
    "DecisionTree(X,Y,np.zeros(4),columns_names,target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have a Decision Tree, It looks something like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree](tree.png \"Decision Tree\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeNode:\n",
    "    def __init__(self):\n",
    "        self.level = -1\n",
    "        self.count = [] # Count of different target values, At index 0 -> count of Setosa \n",
    "                        # At index 1 -> count of versicolor, At index 2 -> count of virginica\n",
    "        self.isLeaf = False # Is the current node a leaf ?\n",
    "        self.entropy = -1 # Entropy of current node\n",
    "        self.splitFeature = -1 # Index of feature to split on\n",
    "        self.splitValue = -1 # The value of the feature to split on\n",
    "        self.left = None # The pointer to the left sub decision tree\n",
    "        self.right = None # The pointer to the right sub decision tree\n",
    "        \n",
    "class DecisionTreeClassifier:\n",
    "    def __init__(self):\n",
    "        self.root = None\n",
    "        # This is the root of the decision tree\n",
    "        \n",
    "    def entropy(self,X,Y,num_targets):\n",
    "        ans = 0\n",
    "        if(Y.shape[0] == 0):\n",
    "            return 0\n",
    "        # Calculating entropy as sum(-pi log2 pi)\n",
    "        total_points = Y.shape[0]\n",
    "        for i in range(num_targets):\n",
    "            if Y[Y==i].shape[0] > 0:\n",
    "                ans -= ((Y[Y==i].shape[0] / total_points) * math.log2 ((Y[Y==i].shape[0] / total_points)))\n",
    "        return ans\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        features_used = np.zeros(X.shape[1]) # Array representing if the i'th feature, is used or not, 1 -> Used, 0 -> Not used\n",
    "        level         = 0\n",
    "        num_targets   = np.unique(Y).shape[0]\n",
    "        self.root     = self.fitHelper(X,Y,features_used,level,num_targets)\n",
    "        \n",
    "    def fitHelper(self,X,Y,features_used,level,num_targets):\n",
    "        # X -> Features\n",
    "        # Y -> Target\n",
    "        # features_used -> 1D array, which is used to specify which feature has been used, 0 is not used, 1 is used\n",
    "        # level -> Current level of the node\n",
    "        \n",
    "        node = DecisionTreeNode()\n",
    "        # We create a tree of a special node, Called decision tree node\n",
    "\n",
    "        node.level = level\n",
    "        \n",
    "        # Let's get the count of all the target values\n",
    "        for i in range(num_targets):\n",
    "            node.count.append(Y[Y==i].shape[0])\n",
    "        \n",
    "        # Setting the entropy of current node\n",
    "        node.entropy = self.entropy(X,Y,num_targets)\n",
    "\n",
    "        if(node.entropy == 0):\n",
    "            # If Entropy = 0, It means leaf node\n",
    "            node.isLeaf = True\n",
    "            return node\n",
    "        \n",
    "        elif(features_used[features_used == 0].shape[0] == 0):\n",
    "            # All the features are used to split and exhausted\n",
    "            node.isLeaf = True\n",
    "            return node\n",
    "        \n",
    "        else:\n",
    "            feature_idx = -1\n",
    "            max_gain_ratio = -1\n",
    "            feature_split = -1\n",
    "            # Go over all the features\n",
    "            for i in range(features_used.shape[0]):\n",
    "                if not features_used[i]:\n",
    "                    # Since the data is continuous, We have to check all the values, And take the maximum of all the values\n",
    "                    # As the values are like 3.4, 3.5, 3.6... We run the loop as 3.35, 3.45, 3.55...\n",
    "                    for j in np.arange(X[:,i].min() - 0.05,  X[:,i].max() + 0.05, 0.1):\n",
    "                        # Splitting on ith index at value j\n",
    "                        # Lets get the left array, and right array\n",
    "                        X_left  = X[X[:,i] < j] \n",
    "                        X_right = X[X[:,i] > j]\n",
    "                        Y_left  = Y[X[:,i] < j]\n",
    "                        Y_right = Y[X[:,i] > j]\n",
    "\n",
    "                        # Getting entropy of both the sides\n",
    "                        entropy_left = self.entropy(X_left,Y_left,num_targets)\n",
    "                        entropy_right = self.entropy(X_right,Y_right,num_targets)\n",
    "\n",
    "                        size_total = Y.shape[0]\n",
    "                        size_left = Y_left.shape[0]\n",
    "                        size_right = Y_right.shape[0]\n",
    "\n",
    "                        # Calculating InfoGain\n",
    "                        entropy_child = (entropy_left * size_left + entropy_right * size_right) / size_total\n",
    "                        current_entropy = self.entropy(X,Y,num_targets)\n",
    "                        info_gain = current_entropy - entropy_child\n",
    "\n",
    "                        # Calculating SplitInfo\n",
    "                        split_info = 0\n",
    "                        if size_left > 0:\n",
    "                            split_info -= (size_left/size_total) * math.log2(size_left/size_total)\n",
    "                        if size_right > 0:\n",
    "                            split_info -= (size_right/size_total) * math.log2(size_right/size_total)\n",
    "                            \n",
    "                        # Calculating Gain Ratio\n",
    "                        if split_info > 0:\n",
    "                            gain_ratio = info_gain/split_info\n",
    "\n",
    "                            if gain_ratio > max_gain_ratio:\n",
    "                                feature_idx = i\n",
    "                                feature_split = j\n",
    "                                max_gain_ratio = gain_ratio\n",
    "\n",
    "            # Splitting the feature at feature_idx index\n",
    "            node.splitFeature = feature_idx\n",
    "            node.splitValue = feature_split\n",
    "\n",
    "            # Since the feature is exhaused, We say that the feature is used\n",
    "            features_used[feature_idx] = 1\n",
    "            \n",
    "            # Recursively build the left Decision Tree, and the right Decision tree\n",
    "            node.left = self.fitHelper(X[X[:,feature_idx] < feature_split], Y[X[:,feature_idx] < feature_split], features_used,level + 1,num_targets)\n",
    "            node.right = self.fitHelper(X[X[:,feature_idx] > feature_split], Y[X[:,feature_idx] > feature_split], features_used,level + 1,num_targets)\n",
    "            \n",
    "            # Setting the feature back to 0, Because we are backtracking now\n",
    "            features_used[feature_idx] = 0\n",
    "            \n",
    "            # The tree is built, So we return the node\n",
    "            return node\n",
    "        \n",
    "    def printTree(self):\n",
    "        self.printTreeHelper(self.root)\n",
    "        return\n",
    "        \n",
    "    def printTreeHelper(self, root):\n",
    "        print(\"Level :\",root.level)\n",
    "        print(\"Count :\",root.count) \n",
    "        print(\"isLeaf :\",root.isLeaf)\n",
    "        print(\"Entropy :\",root.entropy)\n",
    "        if(root.isLeaf):\n",
    "            print()\n",
    "            return\n",
    "        print(\"Split Feature :\",root.splitFeature)\n",
    "        print(\"Split Value\", root.splitValue)\n",
    "        print()\n",
    "        if(root.left):\n",
    "            self.printTreeHelper(root.left)\n",
    "        if(root.right):\n",
    "            self.printTreeHelper(root.right)\n",
    "        return\n",
    "        \n",
    "    def predict(self,X):\n",
    "        # X is a 2d Array, with every row as one data, and colums as features\n",
    "        # ans_y contains the output for the ith data input\n",
    "        ans_y = []\n",
    "        for x in X:\n",
    "            # Predict the answer for every data input, and append it to ans_y\n",
    "            ans_y.append(self.predictHelper(x, self.root))\n",
    "        return np.array(ans_y)\n",
    "    \n",
    "    def predictHelper(self, X, root):\n",
    "        if root.isLeaf:\n",
    "            # if node is leaf, we take the majority vote, i.e. the maximum value\n",
    "            return root.count.index(max(root.count))\n",
    "        else:\n",
    "            # If the node is not a leaf, We check the splitFeature, And check if the value of that \n",
    "            # feature is less than the splitValue And based on that,\n",
    "            # we move to left subtree or right subtree\n",
    "            if(X[root.splitFeature] < root.splitValue):\n",
    "                return self.predictHelper(X, root.left)\n",
    "            else:\n",
    "                return self.predictHelper(X, root.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level : 0\n",
      "Count : [50, 50, 50]\n",
      "isLeaf : False\n",
      "Entropy : 1.584962500721156\n",
      "Split Feature : 2\n",
      "Split Value 1.9500000000000008\n",
      "\n",
      "Level : 1\n",
      "Count : [50, 0, 0]\n",
      "isLeaf : True\n",
      "Entropy : 0.0\n",
      "\n",
      "Level : 1\n",
      "Count : [0, 50, 50]\n",
      "isLeaf : False\n",
      "Entropy : 1.0\n",
      "Split Feature : 3\n",
      "Split Value 1.7500000000000007\n",
      "\n",
      "Level : 2\n",
      "Count : [0, 49, 5]\n",
      "isLeaf : False\n",
      "Entropy : 0.44506485705083865\n",
      "Split Feature : 0\n",
      "Split Value 7.049999999999993\n",
      "\n",
      "Level : 3\n",
      "Count : [0, 49, 4]\n",
      "isLeaf : False\n",
      "Entropy : 0.3860189005698934\n",
      "Split Feature : 1\n",
      "Split Value 2.8499999999999988\n",
      "\n",
      "Level : 4\n",
      "Count : [0, 27, 4]\n",
      "isLeaf : True\n",
      "Entropy : 0.5547781633412736\n",
      "\n",
      "Level : 4\n",
      "Count : [0, 22, 0]\n",
      "isLeaf : True\n",
      "Entropy : 0.0\n",
      "\n",
      "Level : 3\n",
      "Count : [0, 0, 1]\n",
      "isLeaf : True\n",
      "Entropy : 0.0\n",
      "\n",
      "Level : 2\n",
      "Count : [0, 1, 45]\n",
      "isLeaf : False\n",
      "Entropy : 0.15109697051711368\n",
      "Split Feature : 0\n",
      "Split Value 5.949999999999998\n",
      "\n",
      "Level : 3\n",
      "Count : [0, 1, 6]\n",
      "isLeaf : False\n",
      "Entropy : 0.5916727785823275\n",
      "Split Feature : 1\n",
      "Split Value 3.0500000000000007\n",
      "\n",
      "Level : 4\n",
      "Count : [0, 0, 6]\n",
      "isLeaf : True\n",
      "Entropy : 0.0\n",
      "\n",
      "Level : 4\n",
      "Count : [0, 1, 0]\n",
      "isLeaf : True\n",
      "Entropy : 0.0\n",
      "\n",
      "Level : 3\n",
      "Count : [0, 0, 39]\n",
      "isLeaf : True\n",
      "Entropy : 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "feature_names = [\"Petal length\",\"Petal Width\",\"Sepal Length\",\"Sepal Width\"]\n",
    "model.fit(X,Y)\n",
    "model.printTree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predicted = model.predict(X)\n",
    "y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "def score(ypred, ytrue):\n",
    "    # Predicting SCORE : \n",
    "    # Correct Predictions / Total Predictions\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(ypred)):\n",
    "        if(ypred[i] == ytrue[i]):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct/total\n",
    "\n",
    "print(score(y_predicted, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Score is 0.97\n",
    "(It could be overfitting the data as well)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
